{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fb24bae9-ec62-4d9f-a372-13dee77822fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai.vision.all as fai\n",
    "import fastbook as faibook\n",
    "\n",
    "fai.matplotlib.rc('image', cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ef55b6f8-be33-4f6d-9100-ed8349359fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get images and transform them \n",
    "# to data with which we can work in NN, like this:\n",
    "# Image -> [[0,0,255]] -> [[0.0, 0.0, 1.0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dfdafa6c-3f15-4248-8fef-d917d26cfc26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/Users/mxz/.fastai/data/mnist_sample')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_imgs = fai.untar_data(fai.URLs.MNIST_SAMPLE)\n",
    "path_to_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "67a66c64-e1c5-4ed6-b07a-2a9116a0338a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [Path('/Users/mxz/.fastai/data/mnist_sample/valid'),Path('/Users/mxz/.fastai/data/mnist_sample/labels.csv'),Path('/Users/mxz/.fastai/data/mnist_sample/train')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have some directories and files here\n",
    "path_to_imgs.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "11145788-dbb4-47e2-a2a5-f2d3b0836adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/Users/mxz/.fastai/data/mnist_sample/train/3/10.png')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_threes_path = (path_to_imgs/'train'/'3').ls().sorted()\n",
    "train_sevens_path = (path_to_imgs/'train'/'7').ls().sorted()\n",
    "\n",
    "train_threes_path[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5fb431e2-846d-4e8b-8ec0-754416f8503b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APCdP02+1a8W006znu7l+VigjLsQOpwO1dlb/CjVnkS3vNb8O6dqDkKtheaiFn3Hou1QcE8YBPcVg+LfCGqeCtYXS9WEPnvEJlML7lKEkA547qawatafqd/pN2LvTb24s7lQQJbeQowB6jI5r1D4Y6b4OvdXtdf8S+K45NY85phY3W6MeaCdrSTNw3IDcd8ZzyKx/i5pXiiLxMNZ8RC1kjvx/os1nJvh2KOFU8HgEHkc5zXntWdP0681a/hsbC3kubqY7Y4oxlmPXiu68OfBzxPq1yJNVtW0XTI/mnu73CbF74UkEn64HvVn4r+LdI1GDR/C3h2QzaTosXli5JJ858BeM9QAOvck9sV5nU1rdXFjdR3VpPLb3ETbo5YnKOh9QRyDV7UfE2v6vbi31PXNSvYA24RXN3JIufXDEjNZdFf/2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA9UlEQVR4AWNgGGSAEe4e2Upda8b/mwSvnloAF4MyDJY9+Pv31ZG/QPASTS72y8+/u/W4OFi4DiFLsoCV8XEyvCy9BGT++cfAsBlNJ7OICD9YSPvB369eaJJw7s+/X+vgHBQGX+r9vz9qUIRgHO5l74FO3S0H46PQAj9+gTzy4mo+E4o4lKOdlHQBJL9bDJssAwOncTlQ1ge7JAMD4/a/f7uhkhjm///PwHAXl87Qn3//quCQtLn29+9abuySSd///n3EiVVOa9ofYKSYostpFWvZFM//8Pfvr/WS6HIMu0GeB4KjYRhSDAzpYKkXzohUg0URtYQA/HZrR+ekLi0AAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_img = fai.Image.open(train_threes_path[1])\n",
    "example_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "64ecf2b8-5be0-4fed-b896-7e0b51ab33a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to view the numbers that make up this image, \n",
    "# we have to convert it to a NumPy array or a PyTorch tensor\n",
    "example_img_as_numbers = fai.tensor(example_img)\n",
    "\n",
    "# 255 -> more \"bright\", dark, 0 -> more transparent, white\n",
    "# we can visualize those numbers, uncomment code below for it\n",
    "# df = fai.pd.DataFrame(example_img_as_numbers)\n",
    "# df.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dae38a08-02c3-4a8e-85c0-2fc1cb33684e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6131, 28, 28])\n",
      "torch.Size([6265, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "three_tensors = [fai.tensor(fai.Image.open(img)) for img in train_threes_path]\n",
    "seven_tensors = [fai.tensor(fai.Image.open(img)) for img in train_sevens_path]\n",
    "\n",
    "stacked_sevens = fai.torch.stack(seven_tensors).float() / 255\n",
    "stacked_threes = fai.torch.stack(three_tensors).float() / 255\n",
    "\n",
    "print(stacked_threes.size())\n",
    "print(stacked_sevens.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c662607e-9d84-4f7e-808d-0f79c309e770",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width = img_height = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "44e665ed-c981-4396-95c4-1ef0a4a8bdd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12396, 28, 28])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_img_sample_tensors = fai.torch.cat([stacked_threes, stacked_sevens])\n",
    "all_img_sample_tensors.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "10368df3-10d2-464b-8e54-90ee3f84a7cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12396"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_len = all_img_sample_tensors.size()[0]\n",
    "samples_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6789b238-e6d2-4861-983b-2ac9889755b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: why we want to do it?\n",
    "\n",
    "# We want to change samples from a list of matrices (a rank-3 tensor) to a list of vectors (a rank-2 tensor). \n",
    "# We can do this using `view`, which is a PyTorch method that changes the shape of a tensor without changing its contents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2c8eef82-4940-474a-b6f9-4b2ca29fa507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12396, 784])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = all_img_sample_tensors.view(samples_len, img_width * img_height)\n",
    "# instead of samples_len we can use -1 to ask torch tensor to calclulate size for us\n",
    "# \"make this axis as big as necessary to fit all the data\"\n",
    "# all_img_sample_tensors.view(-1, img_width * img_height)\n",
    "train_x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beb90da-eaf2-46ae-93ff-f7304fcb6c40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
